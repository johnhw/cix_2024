{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8th International Summer School on Computational Interaction](imgs/header.png)\n",
    "\n",
    "# 3: Review\n",
    "**John H. Williamson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the exercise\n",
    "\n",
    "* What did you find?\n",
    "* How did different models cope with degraded conditions?\n",
    "* What (if anything!) did you learn about the models?\n",
    "* Was there anything surprising or unexpected?\n",
    "* How did the computational performance of the models compare? \n",
    "\n",
    "# Thoughts\n",
    "\n",
    "* How would you have coped if you needed to predict the **orientation** of the fist as well as the **position**? Or if you needed to also identify the user who was providing input?\n",
    "* How would you integrate a front-facing camera that could also see the fist into the model?\n",
    "* What would you do if faced with a similar problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Blending forward and inverse models\n",
    "\n",
    "We \"walked\" from simple direct inversion to more and more principled Bayesian inverse models. We saw that the Bayesian inverse model is more robust to noise and uncertainty, but also more computationally expensive and it is harder to take advantage of things we often have in abundance: *data* and *deep learning approaches*.\n",
    "\n",
    "We can *combine* some of the benefits of a Bayesian and the traditional ML approach. There are several ways to do this:\n",
    "\n",
    "1. Using ML to learn a *fast approximate forward model* (neural emulation), where we amortise the cost of simulation in the Bayesian model by using a learned model. \n",
    "2. Using ML to learn a latent representation of sensor states (either unsupervised, or as a byproduct from a supervised direct inverse model), and then perform Bayesian inference in this space. For example, we can reduce a camera stream to a small vector of features that are more robust to noise and easier to model.\n",
    "3. Incorporating direct inverse model outputs as evidence sources in a Bayesian inverse model. In this case, we run a direct model first, and then use the output as *part* of the evidence in a Bayesian model. This can be useful when the direct model is fast and accurate, but not robust to noise or uncertainty.\n",
    "4. Use a slow but accurate Bayesian inverse model to *train* a better direct inverse model. This can be done as *active learning* where the Bayesian model is used to generate *good* training data for the inverse model.\n",
    "\n",
    "## Learning a latent representation\n",
    "One way to combine these ideas is to train a neural network to learn a latent representation of the sensor data. This can be done in a supervised way (as we have seen), with some bottleneck layer that forces a simple latent representation, or in a fully unsupervised way (for example, using variational autoencoders). In either case, we learn a much simpler latent space that captures (hopefully) salient elements. \n",
    "\n",
    "We can then apply simulation to generate synthetic observations, put them through the learned latent representation, and then perform likelihood-based inference in this space. This can be used to reduce the dimensionality of the input space, and to make the data more robust to noise.\n",
    "\n",
    "![Combining a forward and inverse model.](imgs/fwd_inv_bottleneck.png)\n",
    "\n",
    "*Combining a forward and inverse model using a learned latent representation.*\n",
    "\n",
    "## Fusing a direct inverse\n",
    "Another way to combine these ideas is to use a direct inverse model as part of the evidence update in a Bayesian model. This can be useful when the direct model is fast and accurate, but not robust to noise or uncertainty. In this case, we run the direct model first, and then use the output as part of the evidence in a Bayesian model. This can be seen as a way to \"smooth\" the output of the direct model, and to make it more robust to noise and uncertainty.\n",
    "\n",
    "If we can make some uncertainty quantification on the direct inverse model (there are various ways we could do this), we can then automatically handover between the direct and Bayesian models depending on the confidence of the direct model. This can be seen as a way to make the direct model adaptively robust to noise and uncertainty. \n",
    "\n",
    "For example, the direct model might be used in regimes where there is lots of training data, and the Bayesian model be more heavily weighted in regimes where there is less data and first-principles modelling is more relevant.\n",
    "\n",
    "![Combining a forward and inverse model.](imgs/fwd_inv.png)\n",
    "*Fusing a direct inverse model in the evidence update of an stochastic filter.*\n",
    "\n",
    "# Reflection\n",
    "What's the summary?\n",
    "\n",
    "* Recovering intention from input is an inverse problem.\n",
    "* Inverse problems are hard because they are underconstrained.\n",
    "* Uncertainty is a key part of the problem.\n",
    "* Direct inversion with a single estimate has a tendency to be brittle.\n",
    "* BUT it can be very effective where data is abundant.\n",
    "* Bayesian models require more \"hand modelling\" but can be more robust to noise and uncertainty.\n",
    "* Probability is a useful universal language to bring together evidence:\n",
    "    * Over time (recursive Bayesian filters)\n",
    "    * Over multiple inputs (sensor fusion)\n",
    "    * Over multiple modalities (e.g. combining with a statistical language model)\n",
    "* (Making interactive demos talk to each other over zeromq is a pain in the ****!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![DIFAI](imgs/difai.png)\n",
    "\n",
    "Supported by the project **Designing Interaction Freedom via Active Inference (DIFAI)**, ERC Advanced Grant proposal 101097708, funded by the UK Horizon guarantee scheme as EPSRC project EP/Y029178/1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
